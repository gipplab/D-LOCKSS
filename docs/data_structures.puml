@startuml D-LOCKSS Data Structures
skinparam classAttributeIconSize 0
skinparam linetype ortho

class ClusterManager {
    - host: host.Host
    - ipfsClient: IPFSClient
    - pubsub: *pubsub.PubSub
    - dht: routing.Routing
    - datastore: datastore.Datastore
    - clusters: map[string]*EmbeddedCluster
    --
    + JoinShard(ctx, shardID, peers)
    + LeaveShard(shardID)
    + Pin(ctx, shardID, cid, ...)
    + Unpin(ctx, shardID, cid)
    + GetAllocations(ctx, shardID, cid)
}

class EmbeddedCluster {
    + ShardID: string
    + Consensus: ConsensusClient (CRDT)
    + PinTracker: *LocalPinTracker
}

class LocalPinTracker {
    - ipfsClient: IPFSClient
    - shardID: string
    --
    + Start(consensus)
    + syncLoop()
    + syncState()
}

ShardManager --> ClusterManager : manages
ClusterManager --> EmbeddedCluster : contains
EmbeddedCluster --> LocalPinTracker : owns

class ShardManager {
    - ctx: context.Context
    - h: host.Host
    - ps: *pubsub.PubSub
    - mu: sync.RWMutex
    - currentShard: string
    - clusterMgr: *ClusterManager
    - shardTopic: *pubsub.Topic
    - shardSub: *pubsub.Subscription
    - oldShardName: string
    - oldShardTopic: *pubsub.Topic
    - oldShardSub: *pubsub.Subscription
    - oldShardEndTime: time.Time
    - inOverlap: bool
    - controlTopic: *pubsub.Topic
    - controlSub: *pubsub.Subscription
    - msgCounter: int
    - lastPeerCheck: time.Time
    - shardDone: chan struct{}
    --
    + NewShardManager(...): *ShardManager
    + Run()
    + AmIResponsibleFor(key): bool  ' key is a content-based routing key (PayloadCID string)
    + PublishToShardCBOR(bytes)
    + PublishToControlCBOR(bytes)
    + splitShard()
    + getShardPeerCount(): int
    + checkAndSplitIfNeeded()
    + runPeerCountChecker()
    + runShardDiscovery()
    + checkAndMergeUpIfAlone()
    + discoverAndMoveToDeeperShard()
    + probeShard(shardID, timeout): int
    + generateDeeperShards(currentShard, maxDepth): []string
    + readOldShard()
    + manageOverlap()
    + Close()
}

note right of ShardManager
  When current shard has few peers (<= MaxPeersPerShard),
  runShardDiscovery runs without requiring idle.
  Merge up: if alone in deep shard (<= 1 peer),
  no siblings (sibling shard empty), parent has room.
  Discover deeper: move to deeper shard if >= 1 peer.
end note

class ResearchObject {
    + MetadataRef: string
    + IngestedBy: peer.ID
    + Signature: []byte
    + Timestamp: int64
    + Payload: cid.Cid
    + TotalSize: uint64
}

class IngestMessage {
    + Type: uint8
    + ManifestCID: cid.Cid
    + ShardID: string
    + HintSize: uint64
    + SenderID: peer.ID
    + Timestamp: int64
    + Nonce: []byte
    + Sig: []byte
}

class ReplicationRequest {
    + Type: uint8
    + ManifestCID: cid.Cid
    + Priority: uint8
    + Deadline: int64
    + SenderID: peer.ID
    + Timestamp: int64
    + Nonce: []byte
    + Sig: []byte
}

class DelegateMessage {
    + Type: uint8
    + ManifestCID: cid.Cid
    + TargetShard: string
    + SenderID: peer.ID
    + Timestamp: int64
    + Nonce: []byte
    + Sig: []byte
}

class StorageState {
    + pinnedFiles: map[string]bool  // ManifestCID strings
    + knownFiles: map[string]bool   // ManifestCID strings
    + fileReplicationLevels: map[string]int
    + fileConvergenceTime: map[string]time.Time
    + recentlyRemoved: map[string]time.Time
    + checkingFiles: map[string]bool
    + lastCheckTime: map[string]time.Time
    + replicationCache: map[string]*cachedReplication
    + pendingVerifications: map[string]*verificationPending
}

class cachedReplication {
    + count: int
    + cachedAt: time.Time
}

class Metrics {
    + pinnedFilesCount: int
    + knownFilesCount: int
    + messagesReceived: int64
    + messagesDropped: int64
    + replicationChecks: int64
    + replicationSuccess: int64
    + replicationFailures: int64
    + shardSplits: int64
    + replicationDistribution: [11]int
    + filesAtTargetReplication: int
    + avgReplicationLevel: float64
    + filesConvergedTotal: int64
    + cumulativeMessagesReceived: int64
    + cumulativeDhtQueries: int64
    + ...
}

class RateLimiter {
    + peers: map[peer.ID]*peerRateLimit
}

class peerRateLimit {
    + messages: []time.Time
    + mu: sync.Mutex
}

class BackoffManager {
    + hashes: map[string]*operationBackoff
}

class operationBackoff {
    + nextRetry: time.Time
    + delay: time.Duration
    + mu: sync.Mutex
}

class PendingVerifications {
    + m: map[string]*verificationPending
}

class verificationPending {
    + firstCount: int
    + firstCheckTime: time.Time
    + verifyTime: time.Time
    + responsible: bool
    + pinned: bool
}

ShardManager --> StorageState : reads/writes
StorageState --> PendingVerifications : contains
PendingVerifications --> verificationPending : contains
StorageState --> Metrics : updates
RateLimiter --> peerRateLimit : contains
BackoffManager --> operationBackoff : contains

@enduml
