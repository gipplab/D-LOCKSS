@startuml D-LOCKSS Data Structures
skinparam classAttributeIconSize 0
skinparam linetype ortho

class ShardManager {
    - ctx: context.Context
    - h: host.Host
    - ps: *pubsub.PubSub
    - mu: sync.RWMutex
    - currentShard: string
    - shardTopic: *pubsub.Topic
    - shardSub: *pubsub.Subscription
    - oldShardName: string
    - oldShardTopic: *pubsub.Topic
    - oldShardSub: *pubsub.Subscription
    - oldShardEndTime: time.Time
    - inOverlap: bool
    - controlTopic: *pubsub.Topic
    - controlSub: *pubsub.Subscription
    - msgCounter: int
    - lastPeerCheck: time.Time
    - shardDone: chan struct{}
    --
    + NewShardManager(...): *ShardManager
    + Run()
    + AmIResponsibleFor(key): bool  ' key is a content-based routing key (PayloadCID string)
    + PublishToShardCBOR(bytes)
    + PublishToControlCBOR(bytes)
    + splitShard()
    + getShardPeerCount(): int
    + checkAndSplitIfNeeded()
    + runPeerCountChecker()
    + readOldShard()
    + manageOverlap()
    + Close()
}

class ResearchObject {
    + MetadataRef: string
    + IngestedBy: peer.ID
    + Signature: []byte
    + Timestamp: int64
    + Payload: cid.Cid
    + TotalSize: uint64
}

class IngestMessage {
    + Type: uint8
    + ManifestCID: cid.Cid
    + ShardID: string
    + HintSize: uint64
    + SenderID: peer.ID
    + Timestamp: int64
    + Nonce: []byte
    + Sig: []byte
}

class ReplicationRequest {
    + Type: uint8
    + ManifestCID: cid.Cid
    + Priority: uint8
    + Deadline: int64
    + SenderID: peer.ID
    + Timestamp: int64
    + Nonce: []byte
    + Sig: []byte
}

class DelegateMessage {
    + Type: uint8
    + ManifestCID: cid.Cid
    + TargetShard: string
    + SenderID: peer.ID
    + Timestamp: int64
    + Nonce: []byte
    + Sig: []byte
}

class StorageState {
    + pinnedFiles: map[string]bool  // ManifestCID strings
    + knownFiles: map[string]bool   // ManifestCID strings
    + fileReplicationLevels: map[string]int
    + fileConvergenceTime: map[string]time.Time
    + recentlyRemoved: map[string]time.Time
    + checkingFiles: map[string]bool
    + lastCheckTime: map[string]time.Time
    + replicationCache: map[string]*cachedReplication
    + pendingVerifications: map[string]*verificationPending
}

class cachedReplication {
    + count: int
    + cachedAt: time.Time
}

class Metrics {
    + pinnedFilesCount: int
    + knownFilesCount: int
    + messagesReceived: int64
    + messagesDropped: int64
    + replicationChecks: int64
    + replicationSuccess: int64
    + replicationFailures: int64
    + shardSplits: int64
    + replicationDistribution: [11]int
    + filesAtTargetReplication: int
    + avgReplicationLevel: float64
    + filesConvergedTotal: int64
    + cumulativeMessagesReceived: int64
    + cumulativeDhtQueries: int64
    + ...
}

class RateLimiter {
    + peers: map[peer.ID]*peerRateLimit
}

class peerRateLimit {
    + messages: []time.Time
    + mu: sync.Mutex
}

class BackoffManager {
    + hashes: map[string]*operationBackoff
}

class operationBackoff {
    + nextRetry: time.Time
    + delay: time.Duration
    + mu: sync.Mutex
}

class PendingVerifications {
    + m: map[string]*verificationPending
}

class verificationPending {
    + firstCount: int
    + firstCheckTime: time.Time
    + verifyTime: time.Time
    + responsible: bool
    + pinned: bool
}

ShardManager --> StorageState : reads/writes
StorageState --> PendingVerifications : contains
PendingVerifications --> verificationPending : contains
StorageState --> Metrics : updates
RateLimiter --> peerRateLimit : contains
BackoffManager --> operationBackoff : contains

@enduml
