@startuml D-LOCKSS Data Structures
skinparam classAttributeIconSize 0
skinparam linetype ortho

class ClusterManager {
    - host: host.Host
    - ipfsClient: IPFSClient
    - pubsub: *pubsub.PubSub
    - dht: routing.Routing
    - datastore: datastore.Datastore
    - trustedPeers: []peer.ID
    - onPinSynced: func(cid string)
    - onPinRemoved: func(cid string)
    - peerProvider: ShardPeerProvider
    - clusters: map[string]*EmbeddedCluster
    --
    + JoinShard(ctx, shardID, bootstrapPeers)
    + LeaveShard(shardID)
    + Pin(ctx, shardID, cid, repMin, repMax)
    + Unpin(ctx, shardID, cid)
    + ListPins(ctx, shardID)
    + GetAllocations(ctx, shardID, cid)
    + GetPeerCount(ctx, shardID)
    + MigratePins(ctx, fromShard, toShard)
    + TriggerSync(shardID)
    + SetShardPeerProvider(provider)
    + GetClusterMetrics(ctx)
}

class EmbeddedCluster {
    + ShardID: string
    + Consensus: ConsensusClient (CRDT)
    + PinTracker: *LocalPinTracker
}

class LocalPinTracker {
    - ipfsClient: IPFSClient
    - shardID: string
    - onPinSynced: func(cid string)
    - onPinRemoved: func(cid string)
    - pinnedByUs: map[string]struct{}
    --
    + Start(consensusClient)
    + TriggerSync()
    + Stop()
    + syncLoop(consensus)
    + syncState(consensus)  ' allocation-aware: pin only if we are in Allocations
}

ShardManager --> ClusterManager : manages
ClusterManager --> EmbeddedCluster : contains
EmbeddedCluster --> LocalPinTracker : owns

class ShardManager {
    - ctx: context.Context
    - h: host.Host
    - ps: *pubsub.PubSub
    - ipfsClient: ipfs.IPFSClient
    - storageMgr: *StorageManager
    - clusterMgr: ClusterManagerInterface
    - metrics: *telemetry.MetricsManager
    - signer: *signing.Signer
    - rateLimiter: *common.RateLimiter
    - mu: sync.RWMutex
    - currentShard: string
    - shardSubs: map[string]*shardSubscription
    - probeTopicCache: map[string]*pubsub.Topic
    - seenPeers: map[string]map[peer.ID]time.Time
    - observerOnlyShards: map[string]struct{}
    - lastPeerCheck: time.Time
    - lastDiscoveryCheck: time.Time
    - lastMoveToDeeperShard: time.Time
    - lastMergeUpTime: time.Time
    - lastShardMove: time.Time
    - lastProbeResponseTime: time.Time
    --
    + NewShardManager(...): *ShardManager
    + Run()
    + JoinShard(shardID)
    + LeaveShard(shardID)
    + JoinShardAsObserver(shardID): bool
    + LeaveShardAsObserver(shardID)
    + AmIResponsibleFor(key): bool  ' PayloadCID string -> shard prefix match
    + PublishToShardCBOR(bytes, shardID)
    + PinToCluster(ctx, cid)
    + EnsureClusterForShard(ctx, shardID)
    + GetPeersForShard(shardID): []peer.ID  ' ShardPeerProvider for CRDT
    + splitShard()
    + getShardPeerCount(): int
    + checkAndSplitIfNeeded()
    + runPeerCountChecker()
    + runShardDiscovery()
    + checkAndMergeUpIfAlone()
    + discoverAndMoveToDeeperShard()
    + probeShard(shardID, timeout): int
    + RunReshardPass(oldShard, newShard)
    + RunOrphanUnpinPass()
    + AnnouncePinned(manifestCID)
    + Close()
}

note right of ShardManager
  PubSub topics: dlockss-creative-commons-shard-<id>.
  One subscription per shard (refCount); observer mode for probing.
  Merge up: understaffed in deep shard, parent has room.
  Discovery: all shards; 10s root, 2m default, 45s when SPLIT known.
  Split rebroadcast: nodes in children rebroadcast SPLIT to ancestors (60s).
  Stability (v0.0.3):
  - lastShardMove: general cooldown (30s) after ANY transition
  - lastProbeResponseTime: rate-limits PROBE heartbeat responses (5s)
  - Jittered discovery timers (0-25% random)
  - Immediate LEAVE on departure
end note

class ResearchObject {
    + MetadataRef: string
    + IngestedBy: peer.ID
    + Signature: []byte
    + Timestamp: int64
    + Payload: cid.Cid
    + TotalSize: uint64
}
note right of ResearchObject
  pkg/schema/types.go
  No Title/Authors/References in code.
end note

class IngestMessage {
    + Type: uint8
    + ManifestCID: cid.Cid
    + ShardID: string
    + HintSize: uint64
    + SenderID: peer.ID
    + Timestamp: int64
    + Nonce: []byte
    + Sig: []byte
}

class ReplicationRequest {
    + Type: uint8
    + ManifestCID: cid.Cid
    + Priority: uint8
    + Deadline: int64
    + SenderID: peer.ID
    + Timestamp: int64
    + Nonce: []byte
    + Sig: []byte
}

class StorageManager {
    - dht: DHTProvider
    - pinnedFiles: *common.PinnedSet
    - knownFiles: *common.KnownFiles
    - recentlyRemoved: *common.RecentlyRemoved
    - fileReplicationLevels: *common.FileReplicationLevels
    - failedOperations: *common.BackoffTable
    - metrics: *telemetry.MetricsManager
    --
    + PinFile(manifestCIDStr)
    + UnpinFile(key)
    + IsPinned(key): bool
    + AddKnownFile(key)
    + GetPinnedManifests(): []string
    + GetKnownFiles(): *KnownFiles
    + GetNextFileToAnnounce(): string
    + ProvideFile(ctx, manifestCIDStr)
}

class cachedReplication {
    + count: int
    + cachedAt: time.Time
}

class Metrics {
    + pinnedFilesCount: int
    + knownFilesCount: int
    + messagesReceived: int64
    + messagesDropped: int64
    + replicationChecks: int64
    + replicationSuccess: int64
    + replicationFailures: int64
    + shardSplits: int64
    + replicationDistribution: [11]int
    + filesAtTargetReplication: int
    + avgReplicationLevel: float64
    + filesConvergedTotal: int64
    + cumulativeMessagesReceived: int64
    + cumulativeDhtQueries: int64
    + ...
}

class RateLimiter {
    + peers: map[peer.ID]*peerRateLimit
}

class peerRateLimit {
    + messages: []time.Time
    + mu: sync.Mutex
}

class BackoffTable {
    + m: map[string]*operationBackoff
}

class operationBackoff {
    + nextRetry: time.Time
    + delay: time.Duration
    + mu: sync.Mutex
}

class PendingVerifications {
    + m: map[string]*verificationPending
}

class verificationPending {
    + firstCount: int
    + firstCheckTime: time.Time
    + verifyTime: time.Time
    + responsible: bool
    + pinned: bool
}

ShardManager --> StorageManager : GetPinnedManifests, AddKnownFile, IsPinned, GetPinTime
StorageManager --> PinnedSet : pinnedFiles
StorageManager --> KnownFiles : knownFiles
StorageManager --> Metrics : updates

class PinnedSet {
    + m: map[string]time.Time
    + Add(key), Remove(key), Has(key), GetPinTime(key), All()
}

class KnownFiles {
    + m: map[string]bool
    + Add(key), Remove(key), Has(key), All()
}
RateLimiter --> peerRateLimit : contains
BackoffTable --> operationBackoff : contains

@enduml
